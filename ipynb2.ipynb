{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "from transformers import AutoTokenizer\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wayne/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def extract_exs(in_lines):\n",
    "    buffer = []\n",
    "    c = 0\n",
    "    while c < len(in_lines):\n",
    "        line = in_lines[c]\n",
    "        if line.startswith('id'):\n",
    "            docid = line[3:].strip()\n",
    "            next_line = in_lines[c+1]\n",
    "            trigger_info = ast.literal_eval(next_line[9:])\n",
    "            buffer.append([docid, trigger_info])\n",
    "        \n",
    "        c += 1\n",
    "\n",
    "    buffers = {tup[0] : {'outputs': tup[1]} for tup in buffer}\n",
    "\n",
    "    return buffers\n",
    "\n",
    "def align_exs(ref_tanls, ref_ogs):\n",
    "    buffers = []\n",
    "    for tanl_ex in ref_tanls:\n",
    "        ref_og = list(filter(lambda info : info['docid'] == tanl_ex['id'], ref_ogs.values()))[0]\n",
    "        buffers.append((tanl_ex, ref_og))\n",
    "    \n",
    "    return buffers\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def span_in_offsets(coref_span, offsets):\n",
    "    coref_span, start_ind = coref_span\n",
    "    return any(tup[0] <= start_ind <= tup[1] for tup in offsets) and any(tup[0] <= start_ind + len(coref_span) <= tup[1] for tup in offsets)\n",
    "\n",
    "for dataset in ['MUC']:\n",
    "    for trigger_source in ['human']:\n",
    "        for split in ['train', 'dev', 'test']:\n",
    "            with open(f'datasets/{dataset}/{trigger_source}/{split}.json', 'r') as f:\n",
    "                gtt_refs = json.loads(f.read())\n",
    "            \n",
    "            for ex in gtt_refs.values():\n",
    "                offsets = tokenizer(\n",
    "                    ex['doctext'],\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_tensors='pt',\n",
    "                    return_offsets_mapping=True\n",
    "                )['offset_mapping'][0]\n",
    "\n",
    "                truncated_templates = []\n",
    "                for og_template in ex['templates']:\n",
    "                    include_template = len(og_template['Triggers']) == 0\n",
    "                    for trigger_corefs in og_template['Triggers']:\n",
    "                        if any(span_in_offsets(trigger_tup, offsets) for trigger_tup in trigger_corefs):\n",
    "                            include_template = True\n",
    "\n",
    "                    if include_template:\n",
    "                        template_copy = {\n",
    "                            'incident_type': og_template['incident_type']\n",
    "                        }\n",
    "                        for role, entity_lst in og_template.items():\n",
    "                            if role != 'incident_type':\n",
    "                                entities = []\n",
    "                                for coref_lst in entity_lst:\n",
    "                                    filtered_coref_lst = []\n",
    "                                    for coref_span, start_ind in coref_lst:\n",
    "                                        if span_in_offsets([coref_span, start_ind], offsets):\n",
    "                                            filtered_coref_lst.append([coref_span, start_ind])\n",
    "                                    \n",
    "                                    if len(filtered_coref_lst):\n",
    "                                        entities.append(filtered_coref_lst)\n",
    "                                \n",
    "                                template_copy[role] = entities\n",
    "                        truncated_templates.append(template_copy)\n",
    "\n",
    "                ex['templates'] = truncated_templates\n",
    "            \n",
    "            with open(f'datasets/{dataset}/{trigger_source}/{split}_trimmed.json', 'w') as f:\n",
    "                f.write(json.dumps(gtt_refs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
