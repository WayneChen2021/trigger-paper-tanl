{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from itertools import islice\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_templates_split(dataset):\n",
    "    \"\"\"\n",
    "    Maps num templates to document id\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    for id, ex in dataset.items():\n",
    "        num_templates = len(ex['templates'])\n",
    "        if not num_templates in counts:\n",
    "            counts[num_templates] = [id]\n",
    "        else:\n",
    "            counts[num_templates].append(id)\n",
    "    \n",
    "    return counts\n",
    "\n",
    "def doc_len_split(dataset):\n",
    "    \"\"\"\n",
    "    Maps document length to document id\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    for id, ex in dataset.items():\n",
    "        doc_len = len(ex['doctext'])\n",
    "        if not doc_len in counts:\n",
    "            counts[doc_len] = [id]\n",
    "        else:\n",
    "            counts[doc_len].append(id)\n",
    "    \n",
    "    return counts\n",
    "\n",
    "def argument_spread_split(dataset):\n",
    "    \"\"\"\n",
    "    Maps argument spread to [document id, template index]\n",
    "    \"\"\"\n",
    "    spreads = {}\n",
    "    for id, ex in dataset.items():\n",
    "        for template_ind, template in enumerate(ex['templates']):\n",
    "            locations = []\n",
    "            for role, entities in template.items():\n",
    "                if role not in ['incident_type', 'Triggers']:\n",
    "                    for coref_mentions in entities:\n",
    "                        locations += [tup[1] for tup in coref_mentions]\n",
    "            \n",
    "            if len(locations) != 0:\n",
    "                location_avg = sum(locations) / len(locations)\n",
    "                spread = sum([(loc - location_avg) ** 2 for loc in locations]) / len(locations)\n",
    "\n",
    "                if not spread in spreads:\n",
    "                    spreads[spread] = [[id, template_ind]]\n",
    "                else:\n",
    "                    spreads[spread].append([id, template_ind])\n",
    "    \n",
    "    return spreads\n",
    "\n",
    "def template_ordering_split(dataset):\n",
    "    \"\"\"\n",
    "    Maps template index to [document id, template index]; index is determined by\n",
    "    mean argument location; templates with no arguments are \"last\"\n",
    "    \"\"\"\n",
    "    orderings = {}\n",
    "    for id, ex in dataset.items():\n",
    "        template_locations = []\n",
    "        for template_ind, template in enumerate(ex['templates']):\n",
    "            locations = []\n",
    "            for role, entities in template.items():\n",
    "                if role not in ['incident_type', 'Triggers']:\n",
    "                    for coref_mentions in entities:\n",
    "                        locations += [tup[1] for tup in coref_mentions]\n",
    "            \n",
    "            if len(locations) != 0:\n",
    "                location_avg = sum(locations) / len(locations)\n",
    "                template_locations.append([template_ind, location_avg])\n",
    "            else:\n",
    "                template_locations.append([template_ind, len(ex['doctext'])])\n",
    "        \n",
    "        template_locations = sorted(template_locations, key=lambda tup : tup[1])\n",
    "        for order_ind, (template_ind, _) in enumerate(template_locations):\n",
    "            if not order_ind in orderings:\n",
    "                orderings[order_ind] = [[id, template_ind]]\n",
    "            else:\n",
    "                orderings[order_ind].append([id, template_ind])\n",
    "    \n",
    "    return orderings\n",
    "\n",
    "def num_fillers_split(dataset):\n",
    "    \"\"\"\n",
    "    Maps number of fillers to [document id, template index]\n",
    "    \"\"\"\n",
    "    fillers_map = {}\n",
    "    for id, ex in dataset.items():\n",
    "        for template_ind, template in enumerate(ex['templates']):\n",
    "            num_fillers = sum(len(lst) for k, lst in template.items() if not k in ['incident_type', 'Triggers'])\n",
    "            if not num_fillers in fillers_map:\n",
    "                fillers_map[num_fillers] = [[id, template_ind]]\n",
    "            else:\n",
    "                fillers_map[num_fillers].append([id, template_ind])\n",
    "    \n",
    "    return fillers_map\n",
    "\n",
    "def num_entities_split(dataset):\n",
    "    \"\"\"\n",
    "    Maps number of entities to [document id, template index]\n",
    "    \"\"\"\n",
    "    entities_map = {}\n",
    "    for id, ex in dataset.items():\n",
    "        for template_ind, template in enumerate(ex['templates']):\n",
    "            entities = set()\n",
    "            for role, entity_lst in template.items():\n",
    "                if not role in ['incident_type', 'Triggers']:\n",
    "                    for coref_lst in entity_lst:\n",
    "                        coref_lst = sorted(coref_lst, key=lambda tup:tup[1])\n",
    "                        entities.add(json.dumps(coref_lst))\n",
    "            \n",
    "            num_entities = len(entities)\n",
    "            if not num_entities in entities_map:\n",
    "                entities_map[num_entities] = [[id, template_ind]]\n",
    "            else:\n",
    "                entities_map[num_entities].append([id, template_ind])\n",
    "    \n",
    "    return entities_map\n",
    "\n",
    "def group_keys(input_dict, num_groups):\n",
    "    sorted_keys = sorted(input_dict.keys())\n",
    "    group_size = math.ceil(len(input_dict) / num_groups)\n",
    "\n",
    "    grouped_dict = {}\n",
    "    for i in range(0, len(sorted_keys), group_size):\n",
    "        current_group = list(islice(sorted_keys, i, i + group_size))\n",
    "        if current_group:\n",
    "            range_key = f\"{current_group[0]}-{current_group[-1]}\"\n",
    "            group_values = []\n",
    "            for key in current_group:\n",
    "                group_values += input_dict[key]\n",
    "            grouped_dict[range_key] = list(group_values)\n",
    "    \n",
    "    return grouped_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = ['WikiEvent/event_roles_in_template', 'MUC']\n",
    "num_groups = 5\n",
    "\n",
    "for dir in dirs:\n",
    "    if dir == 'MUC':\n",
    "        dataset_output_dir = os.path.join('splits/raw', 'MUC')\n",
    "    else:\n",
    "        dataset_output_dir = os.path.join('splits/raw', 'WikiEvent')\n",
    "    os.mkdir(dataset_output_dir)\n",
    "    os.mkdir(dataset_output_dir.replace('raw', 'bucketed'))\n",
    "    \n",
    "    for split_name, splitter in zip(\n",
    "        ['num_templates', 'doc_len', 'argument_spread', 'template_ordering', 'num_entities'],\n",
    "        [num_templates_split, doc_len_split, argument_spread_split, template_ordering_split, num_entities_split]\n",
    "    ):\n",
    "        output_dir_raw = os.path.join(dataset_output_dir, split_name)\n",
    "        os.mkdir(output_dir_raw)\n",
    "        output_dir_bucketed = os.path.join(dataset_output_dir.replace('raw', 'bucketed'), split_name)\n",
    "        os.mkdir(output_dir_bucketed)\n",
    "        \n",
    "        for dataset_split in ['train', 'dev', 'test']:\n",
    "            with open(os.path.join(os.path.join(dir, 'human'), f'{dataset_split}.json'), 'r') as f:\n",
    "                dataset = json.loads(f.read())\n",
    "        \n",
    "            splits = splitter(dataset)\n",
    "            with open(os.path.join(output_dir_raw, f'{dataset_split}.json'), 'w') as f:\n",
    "                f.write(json.dumps(splits))\n",
    "            \n",
    "            grouped_splits = group_keys(splits, num_groups)\n",
    "            with open(os.path.join(output_dir_bucketed, f'{dataset_split}.json'), 'w') as f:\n",
    "                f.write(json.dumps(grouped_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'MUC'\n",
    "\n",
    "for split in ['train', 'dev', 'test']:\n",
    "    aggregates = {}\n",
    "    for trigger_source in ['human', 'llm', 'keyword']:\n",
    "        with open(f'{dataset_dir}/{trigger_source}/{split}.json') as f:\n",
    "            examples = json.loads(f.read())\n",
    "        \n",
    "        for id, info in examples.items():\n",
    "            for template_id, template in enumerate(info['templates']):\n",
    "                del template['Triggers']\n",
    "                tup = (id, template_id)\n",
    "                if not tup in aggregates:\n",
    "                    aggregates[tup] = [json.dumps(template)]\n",
    "                else:\n",
    "                    aggregates[tup].append(json.dumps(template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup, aggregate in aggregates.items():\n",
    "    assert len(aggregate) == 3 and len(set(aggregate)) == 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
